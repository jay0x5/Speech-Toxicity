{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc32cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip installs\n",
    "!pip install tensorflow tensorflow-gpu pandas matplotlib sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8bc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c23a23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data from csv file\n",
    "df = pd.read_csv(os.path.join('data','train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aac0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the below codeline to print data within the csv file\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab9d60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting comments from \"comment_text\" column\n",
    "comments_x = df['comment_text'] \n",
    "\n",
    "#grab all values after first 2 columns since we dont need id and we already have comment_text in comments_x which is a 1D array variable\n",
    "labels_y = df[df.columns[2:]].values \n",
    "\n",
    "#in a nutshell, comments_x holds all the data comments and labels_y represents corresponding labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9ca7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words to be included in the our vectorization layer(considering reducing value of less vram :D)\n",
    "maxWords = 200000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdfd190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our Text Vectorization layer\n",
    "vectorizer_layer = TextVectorization(max_tokens=maxWords,\n",
    "                               output_sequence_length=1800,\n",
    "                               output_mode='int')\n",
    "\n",
    "vectorizer_layer.adapt(comments_x.values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7258953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the vectorizing layer to our comments data\n",
    "vectorized_data = vectorizer_layer(comments_x.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ee032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our dataset \n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_data, labels_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b81e3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caching the dataset\n",
    "dataset = dataset.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9bc93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling the dataset\n",
    "dataset = dataset.shuffle(160000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff8c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the batch size(please reduce if low vram)\n",
    "dataset = dataset.batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf7be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefetch method sort of helps performance (to read more about it=> https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch)\n",
    "dataset = dataset.prefetch(8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "478618da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing our set into training,testing and valuation set\n",
    "train = dataset.take(int(len(dataset)*.7))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5af141d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model :D\n",
    "\n",
    "#Defining a sequential model\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f46dda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embedding layer \n",
    "model.add(Embedding(maxWords+1, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "777d0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Bidirectional LSTM Layer\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afb77fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Feature extractor Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8208648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the last but not the least, Final layer!!\n",
    "model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cfeaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets compile the model with a Optimizer \"Adam\" and a loss logic \"BinaryCrossEntropy\"\n",
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "266428df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model for 1-epoch for now (feel free to adjust it as per your system specs)\n",
    "# Trained_Model = model.fit(train, epochs=1, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75126d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment and use for saving our trained model with .h5 extension so we can use it outside of this notebook ;)\n",
    "#Trained_Model.save('Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4391ac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input something: asshole\n"
     ]
    }
   ],
   "source": [
    "'''Now lets test our model, since we are trying within the notebook and i do not have enough vram, \n",
    "lets use the model.h5 file which i trained on google colab as our loading point :D'''\n",
    "\n",
    "#loading the model\n",
    "model_path = os.path.join('Data','toxicity.h5')\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "#Time to test!\n",
    "\n",
    "#taking an input\n",
    "test_input = input(\"Input something: \")\n",
    "\n",
    "#putting the input for prediction\n",
    "input_text = vectorizer_layer(test_input)\n",
    "res = model.predict(np.expand_dims(input_text,0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e64e3db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toxic: False\\nsevere_toxic: False\\nobscene: False\\nthreat: False\\ninsult: False\\nidentity_hate: False\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#processing the result in a more human readable way\n",
    "text = ''\n",
    "for idx, col in enumerate(df.columns[2:]):\n",
    "        text += '{}: {}\\n'.format(col, res[0][idx]>0.5)\n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c236f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
